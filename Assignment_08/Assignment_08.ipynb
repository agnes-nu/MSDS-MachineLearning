{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_08.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmoretz/MSDS-MachineLearning/blob/master/Assignment_08/Assignment_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sBnJGP7kvo9r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Workspace Initialization"
      ]
    },
    {
      "metadata": {
        "id": "7RN7Fkvg5K5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Libraries & Standard Setup"
      ]
    },
    {
      "metadata": {
        "id": "4-e938y6g5aD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qd7U8ykV0Yz0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Visuals\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Text\n",
        "import re  # regular expressions\n",
        "\n",
        "# Url\n",
        "import urllib.parse\n",
        "\n",
        "# Display Images\n",
        "from PIL import Image, ImageFilter\n",
        "\n",
        "# IPython display\n",
        "from IPython.display import display\n",
        "\n",
        "# Loading Zip Files\n",
        "import zipfile\n",
        "\n",
        "# Working with images\n",
        "import cv2\n",
        "\n",
        "# Progress Bar (for long processes)\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSK_OCczKNiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q chakin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qULKb8UEuv_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import chakin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HB1BMupUKi2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*lists available indices in English*"
      ]
    },
    {
      "metadata": {
        "id": "YgfjTeViKeOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "06b7f0e7-7883-4382-e75e-8eb59e73dd2c"
      },
      "cell_type": "code",
      "source": [
        "chakin.search(lang='English')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Name  Dimension                     Corpus VocabularySize  \\\n",
            "2          fastText(en)        300                  Wikipedia           2.5M   \n",
            "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
            "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
            "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
            "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
            "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
            "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
            "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
            "\n",
            "      Method Language    Author  \n",
            "2   fastText  English  Facebook  \n",
            "11     GloVe  English  Stanford  \n",
            "12     GloVe  English  Stanford  \n",
            "13     GloVe  English  Stanford  \n",
            "14     GloVe  English  Stanford  \n",
            "15     GloVe  English  Stanford  \n",
            "16     GloVe  English  Stanford  \n",
            "17     GloVe  English  Stanford  \n",
            "18     GloVe  English  Stanford  \n",
            "19     GloVe  English  Stanford  \n",
            "20     GloVe  English  Stanford  \n",
            "21  word2vec  English    Google  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5dMHt_ay2-2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Globals"
      ]
    },
    {
      "metadata": {
        "id": "dFsyVgdVKt8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Specify English embeddings file to download and install by index number, number of dimensions, and subfoder name\n",
        "- Note that GloVe 50-, 100-, 200-, and 300-dimensional folders are downloaded with a single zip download"
      ]
    },
    {
      "metadata": {
        "id": "DE6d4wBwToYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "\n",
        "CHAKIN_INDEX = 11\n",
        "NUMBER_OF_DIMENSIONS = 50\n",
        "SUBFOLDER_NAME = \"gloVe.6B\"\n",
        "\n",
        "EMBEDDINGS_FOLDER = \"/content/drive/My Drive/datasets/movie_reviews/embeddings/gloVe.6B/\"\n",
        "EMBEDDINGS_FILE_50 = \"glove.6B.50d.txt\"\n",
        "EMBEDDINGS_FILE_100 = \"glove.6B.50d.txt\"\n",
        "\n",
        "MOVE_REVIEWS = \"/content/drive/My Drive/datasets/movie_reviews/\"\n",
        "MOVE_REVIEWS_POS = \"movie-reviews-positive\"\n",
        "MOVE_REVIEWS_NEG = \"movie-reviews-negative\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BE-2wIPZMag4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- No stopword removal"
      ]
    },
    {
      "metadata": {
        "id": "CjAnhG18MZEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REMOVE_STOPWORDS = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zz_9Q_odMf7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Specify desired size of pre-defined embedding vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "swCjAzB-MeK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EVOCABSIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p94aNZpc-c46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to make output stable across runs."
      ]
    },
    {
      "metadata": {
        "id": "kLJPGcAB-Zkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QgN3nvPvzkD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Project Dataset"
      ]
    },
    {
      "metadata": {
        "id": "v2lhUWKH2fzP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embeddings Dataset"
      ]
    },
    {
      "metadata": {
        "id": "xd1fKDr9JZPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the \"movie reviews\" dataset from mounted drive."
      ]
    },
    {
      "metadata": {
        "id": "GDCBmh7UPVI0",
        "colab_type": "code",
        "outputId": "248d34eb-42b4-4070-a812-e7130d3425f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9JFswvH6kP-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility function for loading embeddings.\n",
        "\n",
        "- Creates the Python defaultdict dictionary **word_to_embedding_dict** for the requested pre-trained word embeddings\n",
        "\n",
        "Note:\n",
        "\n",
        "- The use of defaultdict data structure from the Python Standard Library collections_defaultdict.py lets the caller specify a default value up front \n",
        "- The default value will be retuned if the key is not a known dictionary key. That is, unknown words are represented by a vector of zeros.\n",
        "- For word embeddings, this default value is a vector of zeros\n",
        "- Documentation for the Python standard library:"
      ]
    },
    {
      "metadata": {
        "id": "Vk6nB3S2PRgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1doO2DKk-Hs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_file_50 = urllib.parse.urljoin(EMBEDDINGS_FOLDER, EMBEDDINGS_FILE_50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TD7sDfxSmARF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77250227-ade3-48ce-b0cd-0add2720e021"
      },
      "cell_type": "code",
      "source": [
        "display('Loading embeddings from: {}'.format(embedding_file_50))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Loading embeddings from: /content/drive/My Drive/datasets/movie_reviews/embeddings/gloVe.6B/glove.6B.50d.txt'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "17HkA0CNmzCX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embedding_file_50, with_indexes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTt2-ppWqKJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc10af18-60a4-477f-9517-6442e3e265bb"
      },
      "cell_type": "code",
      "source": [
        "display('Loaded Word to Index: {}, Indexes: {}'.format(len(word_to_index), len(index_to_embedding)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Loaded Word to Index: 400000, Indexes: 400001'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PWKHnRIm1xvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Set Exploration"
      ]
    },
    {
      "metadata": {
        "id": "SHQ9t9dyuzxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size, embedding_dim = index_to_embedding.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsCPXqU3qsRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ad82b17-2b6b-4c19-95fb-8f380f38c847"
      },
      "cell_type": "code",
      "source": [
        "display(\"Embedding is of shape: {}\".format(index_to_embedding.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Embedding is of shape: (400001, 50)'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zxr-PGC0q7EC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This means (number of words, number of dimensions per word)\n",
        "The first words are words that tend occur more often.\n",
        "\n",
        "Note: \n",
        "- for unknown words, the representation is an empty vector, and the index is the last one. The dictionnary has a limit:"
      ]
    },
    {
      "metadata": {
        "id": "ab-M-wi-rKGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c5707ef-9bac-4e9b-cbbf-6f7a51184e25"
      },
      "cell_type": "code",
      "source": [
        "display(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'    A word --> Index in embedding --> Representation'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nhnMc9YQrP2n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sanity Checks of the Data"
      ]
    },
    {
      "metadata": {
        "id": "NRf8MFsFrwXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word_index(word):\n",
        "  idx = word_to_index[word]\n",
        "  complete_vocabulary_size = idx\n",
        "  embd = list(np.array(index_to_embedding[idx], dtype=int))\n",
        "  return idx, embd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ihE-Hsbrcw4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A word obviously not in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "90rPUutJrgsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f345eab2-50c2-4b97-d72e-3446c831708b"
      },
      "cell_type": "code",
      "source": [
        "word = \"worsdfkljsdf\"\n",
        "\n",
        "idx, embd = get_word_index(word)\n",
        "display(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_CtyrtHf1Kxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save the index as the word wasn't found, it denotes the full vocabulary size."
      ]
    },
    {
      "metadata": {
        "id": "BuuP8RNF1IpF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "complete_vocabulary_size = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4bkS5ASsIgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A word that should be in the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "FqIL8NmfsN4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bd34831-5043-4754-d308-5631bb1d0f6c"
      },
      "cell_type": "code",
      "source": [
        "word = \"the\"\n",
        "\n",
        "idx, embd = get_word_index(word)\n",
        "display(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'    the --> 0 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ecjIzOF5dnu6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentence Testing"
      ]
    },
    {
      "metadata": {
        "id": "21yam3EL-wxF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's test a simple sentence and <a href=\"https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\">famous phrase</a>."
      ]
    },
    {
      "metadata": {
        "id": "39v2KcALtjjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c83490bc-9731-4434-89f6-60376f93a4e0"
      },
      "cell_type": "code",
      "source": [
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "display('Test sentence: {}'.format(a_typing_test_sentence))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Test sentence: The quick brown fox jumps over the lazy dog'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ESk3kYa3txYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4ae39ee-18ed-4c97-e84e-ecba36a3ab7b"
      },
      "cell_type": "code",
      "source": [
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "display('{}'.format(words_in_test_sentence))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\""
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "paPOFrpyt3Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54058b61-a64a-4e3c-dab2-967d788ff06b"
      },
      "cell_type": "code",
      "source": [
        "display('Test sentence embeddings from complete vocabulary of {} words.'.format(complete_vocabulary_size))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Test sentence embeddings from complete vocabulary of 400000 words.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "T4YgTZq0uF0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1343
        },
        "outputId": "246f4d28-dce3-4c1a-fe09-463a4722fc4e"
      },
      "cell_type": "code",
      "source": [
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
            "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
            " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
            " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
            "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
            "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
            " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
            "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
            "  0.55305   -0.0028062]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
            " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
            " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
            "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
            "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
            " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
            " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
            "  1.1672  ]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o2MV3MnYrTZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "A-Grmx6x2XOL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define vocabulary size for the language model, to reduce the size of the vocabulary to the n most frequently used words"
      ]
    },
    {
      "metadata": {
        "id": "bzMULN5buoBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "last/unknown-word row in limited_index_to_embedding"
      ]
    },
    {
      "metadata": {
        "id": "hEFsQ0MHUQtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def default_factory():\n",
        "    return EVOCABSIZE "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYPsJAgzus-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "dictionary has the items() function, returns list of (key, value) tuples"
      ]
    },
    {
      "metadata": {
        "id": "SRT9eh_8uuE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KinUGdqtw7qB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Select the first EVOCABSIZE rows to the index_to_embedding."
      ]
    },
    {
      "metadata": {
        "id": "ZJDwNuJKuw8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDpLBkU3w1TZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set the unknown-word row to be all zeros as previously."
      ]
    },
    {
      "metadata": {
        "id": "XXGdg9Zzw0Nk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RX8dZKxxAfv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Delete large numpy array to clear some CPU RAM."
      ]
    },
    {
      "metadata": {
        "id": "RKUAo8lAw-ZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKJcJXDuxRf-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Verify the new vocabulary: should get same embeddings for test sentence\n",
        "\n",
        "Note:\n",
        "- that a small EVOCABSIZE may yield some zero vectors for embeddings"
      ]
    },
    {
      "metadata": {
        "id": "fnlDCZOhxYn4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "83cf9460-7fd3-4f98-8740-725e3be6b2a1"
      },
      "cell_type": "code",
      "source": [
        "display('\\nTest sentence embeddings from vocabulary of {} words.'.format(EVOCABSIZE))\n",
        "\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    display(' word: {} : embedding: {}'.format( word_, embedding))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'\\nTest sentence embeddings from vocabulary of 10000 words.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: the : embedding: [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\\n -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\\n -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\\n -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\\n -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\\n  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\\n  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\\n -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\\n -1.1514e-01 -7.8581e-01]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: quick : embedding: [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\\n -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\\n -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\\n -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\\n  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\\n  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\\n  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\\n -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\\n  0.57892    0.64483  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: brown : embedding: [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\\n -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\\n  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\\n  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\\n  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\\n -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\\n -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\\n  0.73274 ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: fox : embedding: [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\\n -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\\n  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\\n  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\\n  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\\n -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\\n -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\\n  1.5064  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: jumps : embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0.]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: over : embedding: [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\\n -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\\n -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\\n -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\\n -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\\n  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\\n -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\\n  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\\n -0.60515   -0.9827   ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: the : embedding: [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\\n -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\\n -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\\n -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\\n -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\\n  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\\n  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\\n -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\\n -1.1514e-01 -7.8581e-01]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: lazy : embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n 0. 0.]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' word: dog : embedding: [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\\n -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\\n  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\\n -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\\n  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\\n  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\\n  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\\n -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\\n  0.7158     0.38519  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GMUqgfEWreWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Enhance Dataset with Stopwords"
      ]
    },
    {
      "metadata": {
        "id": "RHu6UDNaz2pp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EmqKVZt50Dps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ada469ad-77e7-454f-e3c7-d546047b0727"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "kv8YY9rFyHSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "define list of codes to be dropped from document\n",
        "- carriage-returns\n",
        "- line-feeds\n",
        "- tabs"
      ]
    },
    {
      "metadata": {
        "id": "j1qlLgJ4yMY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "codelist = ['\\r', '\\n', '\\t']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aY3ZGh7PzRRX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will not remove stopwords in this exercise because they are important to keeping sentences intact"
      ]
    },
    {
      "metadata": {
        "id": "gYo6zkchzQe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZAU-fFQzW0K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Previous analysis of a list of top terms showed a number of words, along with contractions and other word strings to drop from further analysis, add these to the usual English stopwords to be dropped from a document collection."
      ]
    },
    {
      "metadata": {
        "id": "1xH361EkzVT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OurHQOBkzn9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nIZZjIxBztJh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start with the initial list and add to it for movie text work."
      ]
    },
    {
      "metadata": {
        "id": "nHiUX60PzrGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cwc_8aUY0PsO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text Parsing"
      ]
    },
    {
      "metadata": {
        "id": "8A-uQF_60Wv2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text parsing function for creating text documents, there is more we could do for data preparation:\n",
        "- stemming\n",
        "- looking for contractions\n",
        "- possessives... \n",
        "\n",
        "but we will work with what we have in this parsing function.Iif we want to do stemming at a later time, we can use\n",
        "**porter = nltk.PorterStemmer()**\n",
        "\n",
        "in a construction like this:\n",
        "\n",
        "**words_stemmed =  [porter.stem(word) for word in initial_words]  **"
      ]
    },
    {
      "metadata": {
        "id": "wo6z9pHv0ToM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHqCRw-z2QOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Reviews"
      ]
    },
    {
      "metadata": {
        "id": "QSOJDxqa-EOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility function to get file names within a directory."
      ]
    },
    {
      "metadata": {
        "id": "_B1Gy1DvzqqF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2v_H3bJs734b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def assert_files_exist(dir_name):\n",
        "  \n",
        "  display('Directory: {}'.format(dir_name))  \n",
        "  \n",
        "  file_names = listdir_no_hidden(path=dir_name)\n",
        "  \n",
        "  display('files found {}'.format(len(filenames)))\n",
        "  \n",
        "  for i in range(len(file_names)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, file_names[i]))\n",
        "    assert file_exists\n",
        "      \n",
        "  return file_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ey3ycLLa881m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gather_documents(dir_name, file_names):\n",
        "  documents = []\n",
        "\n",
        "  display('Gathering {} files.'.format(dir_name))\n",
        "\n",
        "  for i in range(num_files):\n",
        "    words = read_data(os.path.join(dir_name, file_names[i]))\n",
        "    documents.append(words)\n",
        "    \n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c6yTzvh39nly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "\n",
        "  with open(filename, encoding='utf-8') as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amdtu1kT2SaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gather negative movie reviews."
      ]
    },
    {
      "metadata": {
        "id": "vTfgDSFf96Cx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Negative Reviews"
      ]
    },
    {
      "metadata": {
        "id": "MIWnbrFp7OG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "neg_dir = urllib.parse.urljoin(MOVE_REVIEWS, MOVE_REVIEWS_NEG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDc2bQSi7Puw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48358622-a8ab-4b35-b6c8-0b311391f04a"
      },
      "cell_type": "code",
      "source": [
        "negative_files = assert_files_exist(neg_dir)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Directory: /content/drive/My Drive/datasets/movie_reviews/movie-reviews-negative'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'files found 500'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "j71hdqxM9KLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b88a0cb-b87d-4608-eec1-65a660c18f1d"
      },
      "cell_type": "code",
      "source": [
        "negative_documents = gather_documents(neg_dir, negative_files)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Gathering /content/drive/My Drive/datasets/movie_reviews/movie-reviews-negative files.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DSllxSAC98lh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Positive Reviews"
      ]
    },
    {
      "metadata": {
        "id": "N2zACisJ9q_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gather positive movie reviews."
      ]
    },
    {
      "metadata": {
        "id": "Cvc6TfDO79As",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_dir = urllib.parse.urljoin(MOVE_REVIEWS, MOVE_REVIEWS_POS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B7bY83Kg9uuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e70c97f-5f8e-4497-c716-c85dc1928727"
      },
      "cell_type": "code",
      "source": [
        "positive_files = assert_files_exist(pos_dir)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Directory: /content/drive/My Drive/datasets/movie_reviews/movie-reviews-positive'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'files found 500'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "d-Rq6Rbj9v8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "756fe0aa-eb58-4ad1-c33c-e347f8f8385b"
      },
      "cell_type": "code",
      "source": [
        "positive_documents = gather_documents(pos_dir, positive_files)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Gathering /content/drive/My Drive/datasets/movie_reviews/movie-reviews-positive files.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gP144X6y_2ju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aggregate"
      ]
    },
    {
      "metadata": {
        "id": "O86gztnOALZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Find max review."
      ]
    },
    {
      "metadata": {
        "id": "3dq_dUTi_4b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "220e094b-b739-4a6e-a82d-0f12f39735c4"
      },
      "cell_type": "code",
      "source": [
        "max_review_length = 0\n",
        "\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))\n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "display('max_review_length: {}'.format(max_review_length))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'max_review_length: 1052'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "J2ICWlE8AGzx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Find min review."
      ]
    },
    {
      "metadata": {
        "id": "nEKL0kicADNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8677c2bf-e2e1-42c8-9044-2cc75ebc7eab"
      },
      "cell_type": "code",
      "source": [
        "min_review_length = max_review_length\n",
        "\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "\n",
        "display('min_review_length: {}'.format(min_review_length)) "
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'min_review_length: 22'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "l4C0i2w_AzSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Construct list of 1000 lists with 40 words in each list"
      ]
    },
    {
      "metadata": {
        "id": "bYRl1A4UArq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "duSDuXOFA5N8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create list of lists of lists for embeddings."
      ]
    },
    {
      "metadata": {
        "id": "P6Og19ecA1ek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kNrSElSNA90f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Validate the reviews."
      ]
    },
    {
      "metadata": {
        "id": "Ua-liUkzBTU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_word(word, place):\n",
        "  display('{} word in first document: {}'.format(place, word))    \n",
        "  display('Embedding for this word: {}'.format(limited_index_to_embedding[limited_word_to_index[word]]))\n",
        "  display('Corresponding embedding from embeddings list of list of lists: {}'.format(embeddings[0][0][:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkWjMl77A_l_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "94105bd0-2259-43d8-f225-9a61ffd2c9f8"
      },
      "cell_type": "code",
      "source": [
        "test_word = documents[0][0]\n",
        "\n",
        "show_word(test_word, 1)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'1 word in first document: story'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Embedding for this word: [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\\n -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\\n -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\\n -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\\n  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\\n  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\\n -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\\n -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\\n  0.23625    0.26451  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Corresponding embedding from embeddings list of list of lists: [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\\n -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\\n -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\\n -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\\n  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\\n  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\\n -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\\n -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\\n  0.23625    0.26451  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AG2AnNYKBQM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f0a0297a-ec95-4739-855a-a9890c6e1bc0"
      },
      "cell_type": "code",
      "source": [
        "test_word = documents[6][9]  \n",
        "\n",
        "show_word(test_word, 7)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'7 word in first document: could'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Embedding for this word: [ 0.90754   -0.38322    0.67648   -0.20222    0.15156    0.13627\\n -0.48813    0.48223   -0.095715   0.18306    0.27007    0.41415\\n -0.48933   -0.0076005  0.79662    1.0989     0.53802   -0.54468\\n -0.16063   -0.98348   -0.19188   -0.2144     0.19959   -0.31341\\n  0.24101   -2.2662    -0.25926   -0.10898    0.66177   -0.48104\\n  3.6298     0.45397   -0.64484   -0.52244    0.042922  -0.16605\\n  0.097102   0.044836   0.20389   -0.46322   -0.46434    0.32394\\n  0.25984    0.40849    0.20351    0.058722  -0.16408    0.20672\\n -0.1844     0.071147 ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Corresponding embedding from embeddings list of list of lists: [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\\n -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\\n -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\\n -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\\n  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\\n  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\\n -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\\n -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\\n  0.23625    0.26451  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u3moTdOQBl_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "47f01ab2-3250-44ba-a846-7b37a7b20d65"
      },
      "cell_type": "code",
      "source": [
        "test_word = documents[999][39]\n",
        "\n",
        "show_word(test_word, 999)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'999 word in first document: well'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Embedding for this word: [ 2.7691e-01  2.8745e-01 -2.9935e-01 -1.9964e-01  1.2956e-01  1.5555e-01\\n -6.4522e-01 -3.4090e-01 -1.1833e-01  1.5798e-01  1.3969e-01  2.4872e-01\\n -1.5901e-01 -3.3439e-02  1.1895e-01  7.6535e-02  4.5263e-01  2.6494e-01\\n -1.9157e-01 -5.6768e-01  2.9286e-02  2.1745e-01  4.3406e-01  1.4981e-01\\n  7.5774e-02 -1.4453e+00 -5.8394e-01 -4.6063e-02  6.6214e-02 -2.6417e-01\\n  3.9650e+00  2.5196e-01  2.4855e-01 -5.0524e-01  2.5806e-01  2.8683e-01\\n -1.7994e-01  6.2885e-01 -1.2040e-01 -4.2143e-02 -4.4911e-02  1.8561e-01\\n  1.6266e-01 -2.6127e-03  1.3083e-01  2.0179e-01 -2.9667e-01 -9.4820e-02\\n -2.1250e-01  2.2074e-02]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Corresponding embedding from embeddings list of list of lists: [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\\n -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\\n -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\\n -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\\n  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\\n  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\\n -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\\n -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\\n  0.23625    0.26451  ]'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XWs-5urnBsm0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GEhYnN6QBw6y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "qeD5to_mCtih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "r91DZrt8CWFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Make embeddings a numpy array for use in an RNN \n",
        "- Create training and test sets with Scikit Learn"
      ]
    },
    {
      "metadata": {
        "id": "t78TOQaYCR7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings_array = np.array(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8ogOZKCCawj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the labels to be used 500 negative (0) and 500 positive (1)"
      ]
    },
    {
      "metadata": {
        "id": "KMERgEBHCZkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Dph3D5ACd9R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scikit Learn for random splitting of the data ."
      ]
    },
    {
      "metadata": {
        "id": "Jv_LLqVLCcDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOwFBFq5Ch2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Random splitting of the data in to training (80%) and test (20%) ."
      ]
    },
    {
      "metadata": {
        "id": "-81nN--qCgMz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j456EtEICkwu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reset the global state."
      ]
    },
    {
      "metadata": {
        "id": "eN8752BzCjO1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YL0-VoPHCoKw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "metadata": {
        "id": "o5CHIzjRC-OG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "number of words per document"
      ]
    },
    {
      "metadata": {
        "id": "U1nL2rMRCqCH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_steps = embeddings_array.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvaqW0_XDDzU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "dimension of  pre-trained embeddings"
      ]
    },
    {
      "metadata": {
        "id": "ElfQiOzuCqYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_inputs = embeddings_array.shape[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nq9lrYOKDGYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "analyst specified number of neurons"
      ]
    },
    {
      "metadata": {
        "id": "0Aah-bxxDBO9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_neurons = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6LP46XZDIYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "thumbs-down or thumbs-up"
      ]
    },
    {
      "metadata": {
        "id": "nZkd100EDChT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_outputs = 2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_KF87E4DKL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Learning Rate."
      ]
    },
    {
      "metadata": {
        "id": "_D_1utV7DLCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWOYAA9VDOCD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Variables"
      ]
    },
    {
      "metadata": {
        "id": "qKP30Qf8DUY_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
        "\n",
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpTy8cIiDUzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rilJg-GUD3Bh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train & Test"
      ]
    },
    {
      "metadata": {
        "id": "0bK40XbuDgK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10217
        },
        "outputId": "55814728-6986-4049-a764-da6b6ad24905"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        display('Train accuracy: {} Test Accuracy: {}'.format(acc_train, acc_test))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  0  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.5699999928474426 Test Accuracy: 0.4749999940395355'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  1  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.5400000214576721 Test Accuracy: 0.5'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  2  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.5699999928474426 Test Accuracy: 0.5049999952316284'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  3  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.5299999713897705 Test Accuracy: 0.5199999809265137'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  4  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.5699999928474426 Test Accuracy: 0.5350000262260437'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  5  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6000000238418579 Test Accuracy: 0.5400000214576721'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  6  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6000000238418579 Test Accuracy: 0.5299999713897705'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  7  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6200000047683716 Test Accuracy: 0.5249999761581421'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  8  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6600000262260437 Test Accuracy: 0.5199999809265137'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  9  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6600000262260437 Test Accuracy: 0.5099999904632568'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  10  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6800000071525574 Test Accuracy: 0.5199999809265137'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  11  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6800000071525574 Test Accuracy: 0.5049999952316284'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  12  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.6899999976158142 Test Accuracy: 0.5049999952316284'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  13  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.699999988079071 Test Accuracy: 0.5099999904632568'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  14  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5099999904632568'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  15  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7200000286102295 Test Accuracy: 0.5199999809265137'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  16  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5299999713897705'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  17  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5299999713897705'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  18  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5450000166893005'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  19  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5400000214576721'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  20  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5450000166893005'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  21  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7200000286102295 Test Accuracy: 0.5550000071525574'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  22  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5649999976158142'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  23  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5699999928474426'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  24  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7099999785423279 Test Accuracy: 0.5699999928474426'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  25  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5799999833106995'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  26  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7400000095367432 Test Accuracy: 0.5799999833106995'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  27  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7400000095367432 Test Accuracy: 0.5849999785423279'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  28  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7400000095367432 Test Accuracy: 0.5799999833106995'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  29  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5849999785423279'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  30  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5899999737739563'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  31  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.5899999737739563'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  32  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7300000190734863 Test Accuracy: 0.6000000238418579'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  33  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7400000095367432 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  34  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.75 Test Accuracy: 0.6000000238418579'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  35  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7400000095367432 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  36  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.75 Test Accuracy: 0.6150000095367432'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  37  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7599999904632568 Test Accuracy: 0.6150000095367432'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  38  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7599999904632568 Test Accuracy: 0.6100000143051147'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  39  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7599999904632568 Test Accuracy: 0.6200000047683716'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  40  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7699999809265137 Test Accuracy: 0.6100000143051147'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  41  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7599999904632568 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  42  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7699999809265137 Test Accuracy: 0.6100000143051147'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  43  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7799999713897705 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  44  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.7799999713897705 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  45  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.800000011920929 Test Accuracy: 0.6100000143051147'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  46  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.800000011920929 Test Accuracy: 0.6050000190734863'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  47  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.8100000023841858 Test Accuracy: 0.6100000143051147'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  48  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.8199999928474426 Test Accuracy: 0.6150000095367432'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  49  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train accuracy: 0.8299999833106995 Test Accuracy: 0.6150000095367432'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MI7N2et1D-Pw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results"
      ]
    },
    {
      "metadata": {
        "id": "i50B0NupEASM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}